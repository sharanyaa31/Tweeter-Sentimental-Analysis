{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9eee992",
   "metadata": {},
   "source": [
    "# Importing all the necessary libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a633e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in ./opt/anaconda3/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in ./opt/anaconda3/lib/python3.8/site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: regex in ./opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: joblib in ./opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "Requirement already satisfied: click in ./opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9c1ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sharanyakarthikeyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sharanyakarthikeyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/sharanyakarthikeyan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/sharanyakarthikeyan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sharanyakarthikeyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "stopword = stopwords.words('english')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import json\n",
    "from nltk.sentiment.util import *\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c6d94",
   "metadata": {},
   "source": [
    "# Import and Read the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b51cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/sharanyakarthikeyan/Downloads/tweets.json') as jfile:\n",
    "  json_file = json.load(jfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aab2e7",
   "metadata": {},
   "source": [
    "# Convert into Data Frame \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb5561e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(json_file).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f05e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_author</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1374140386071961602</th>\n",
       "      <td>Hematopoiesis News</td>\n",
       "      <td>⚕️ Scientists conducted a Phase II study of ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374032432173842437</th>\n",
       "      <td>Michael Wang, MD</td>\n",
       "      <td>This phase 2 Acalabrutinib-Venetoclax (AV) tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373902876553048065</th>\n",
       "      <td>1stOncology</td>\n",
       "      <td>#NICE backs #AstraZenecas #Calquence for #CLL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373656782367813635</th>\n",
       "      <td>Toby Eyre</td>\n",
       "      <td>#acalabrutinib is a valuable option in pts int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372941634334232586</th>\n",
       "      <td>Lymphoma Hub</td>\n",
       "      <td>NICE has recommended the use of acalabrutinib ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372927482278539265</th>\n",
       "      <td>David Ledger</td>\n",
       "      <td>NICE backs AstraZeneca’s Calquence for CLL htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372911723305394179</th>\n",
       "      <td>N Wales Cancer Forum</td>\n",
       "      <td>This is England for now - these decisions usua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372888121159868423</th>\n",
       "      <td>European Pharmaceutical Review</td>\n",
       "      <td>AstraZeneca’s Calquence (acalabrutinib), a che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372866915081797632</th>\n",
       "      <td>Graham Collins</td>\n",
       "      <td>Superstar @tobyeyre82 responding to the excell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372825553837944834</th>\n",
       "      <td>CLL Ireland</td>\n",
       "      <td>CLL patients all know the drug Ibrutinib and y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweet_author  \\\n",
       "1374140386071961602              Hematopoiesis News   \n",
       "1374032432173842437                Michael Wang, MD   \n",
       "1373902876553048065                     1stOncology   \n",
       "1373656782367813635                       Toby Eyre   \n",
       "1372941634334232586                    Lymphoma Hub   \n",
       "1372927482278539265                    David Ledger   \n",
       "1372911723305394179            N Wales Cancer Forum   \n",
       "1372888121159868423  European Pharmaceutical Review   \n",
       "1372866915081797632                  Graham Collins   \n",
       "1372825553837944834                     CLL Ireland   \n",
       "\n",
       "                                                            tweet_text  \n",
       "1374140386071961602  ⚕️ Scientists conducted a Phase II study of ac...  \n",
       "1374032432173842437  This phase 2 Acalabrutinib-Venetoclax (AV) tri...  \n",
       "1373902876553048065  #NICE backs #AstraZenecas #Calquence for #CLL ...  \n",
       "1373656782367813635  #acalabrutinib is a valuable option in pts int...  \n",
       "1372941634334232586  NICE has recommended the use of acalabrutinib ...  \n",
       "1372927482278539265  NICE backs AstraZeneca’s Calquence for CLL htt...  \n",
       "1372911723305394179  This is England for now - these decisions usua...  \n",
       "1372888121159868423  AstraZeneca’s Calquence (acalabrutinib), a che...  \n",
       "1372866915081797632  Superstar @tobyeyre82 responding to the excell...  \n",
       "1372825553837944834  CLL patients all know the drug Ibrutinib and y...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee70fe4",
   "metadata": {},
   "source": [
    "# checking for null values and unique writters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffdca18",
   "metadata": {},
   "source": [
    "print('total no of null valus in the data:\\n',df.isnull().sum())\n",
    "print('total no tweet author:',df.tweet_author.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3405d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.lower()\n",
    "    df = re.sub(r'[^(a-zA-Z)\\s]','',df)\n",
    "    #remove urls\n",
    "    df = re.sub(r'http\\S+', \" \", df)\n",
    "    #remove mentions\n",
    "    df = re.sub(r'@\\w+',' ',df)\n",
    "    #remove hashtags\n",
    "    df= re.sub(r'#\\w+', ' ',df)\n",
    "    #remove digits\n",
    "    df = re.sub(r'\\d+()', ' ', df)\n",
    "    #remove html tags and umber\n",
    "    df = re.sub('r<.*?>',' ', df)\n",
    "    #remove stopwords\n",
    "    df = df.split()\n",
    "    df = \" \".join([word for word in df if not word in stopword])\n",
    "    return df\n",
    "df['tweet_text']=df['tweet_text'].apply(lambda x:clean_data(x))\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "df['tweet_text']= df['tweet_text'].apply(lambda x: remove_punct(x))\n",
    "#Tokenization of the text data\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "df['tweet_text'] = df['tweet_text'].apply(lambda x: tokenization(x.lower()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3929a",
   "metadata": {},
   "source": [
    "# objective 1\n",
    "Get the most frequent entities from the tweets. and we convert them into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf7f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f69e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "df1 = df1.apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7d237fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "df1= df1.apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb574a",
   "metadata": {},
   "source": [
    "# Stanford NLP NER\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "386d5793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-16 12:25:35--  https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-ner-2018-10-16.zip [following]\n",
      "--2022-11-16 12:25:37--  https://downloads.cs.stanford.edu/nlp/software/stanford-ner-2018-10-16.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 180358328 (172M) [application/zip]\n",
      "Saving to: ‘stanford-ner-2018-10-16.zip’\n",
      "\n",
      "stanford-ner-2018-1 100%[===================>] 172.00M  5.53MB/s    in 86s     \n",
      "\n",
      "2022-11-16 12:27:06 (1.99 MB/s) - ‘stanford-ner-2018-10-16.zip’ saved [180358328/180358328]\n",
      "\n",
      "Archive:  stanford-ner-2018-10-16.zip\n",
      "replace stanford-ner-2018-10-16/README.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sharanyakarthikeyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "!wget 'https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip' -O stanford-ner-2018-10-16.zip\n",
    "!unzip stanford-ner-2018-10-16.zip\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49b2d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = StanfordNERTagger('stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                       'stanford-ner-2018-10-16/stanford-ner.jar',\n",
    "                       encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc62617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [item if isinstance(df1, str) else \" \".join(item) for item in df1 ]\n",
    "seen = set()\n",
    "val = [x for x in val if x not in seen and not seen.add(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdcd8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_text = st.tag(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28d445d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity Name</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acalabrutinib</td>\n",
       "      <td>1306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>calquenc</td>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patient</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>astrazeneca</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cll</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trial</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lymphocyt</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chronic</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>leukemia</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Entity Name  Frequency\n",
       "0  acalabrutinib       1306\n",
       "1       calquenc        893\n",
       "2        patient        790\n",
       "3          covid        694\n",
       "4    astrazeneca        598\n",
       "5            cll        562\n",
       "6          trial        425\n",
       "7      lymphocyt        388\n",
       "8        chronic        351\n",
       "9       leukemia        342"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity= pd.DataFrame(classified_text,columns=['Entity Name','Entity Type'])\n",
    "\n",
    "#Here We Remove Entities_type column from data we does not requierd this but the help to recgonized the type of entity\n",
    "\n",
    "all_entities = (entity.groupby(by=['Entity Name'])\n",
    "                          .size()\n",
    "                          .sort_values(ascending=False)\n",
    "                          .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "all_entities.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "305b8c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities.to_csv('objective1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade06cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
